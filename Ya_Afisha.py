#!/usr/bin/env python
# coding: utf-8

# # Анализ лояльности пользователей Яндекс Афиши

# ## Этапы выполнения проекта
# 
# ### 1. Загрузка данных и их предобработка
# 
# ---
# 
# **Задача 1.1:** Напишите SQL-запрос, выгружающий в датафрейм pandas необходимые данные. Используйте следующие параметры для подключения к базе данных `data-analyst-afisha`:
# 
# - **Хост** — `rc1b-wcoijxj3yxfsf3fs.mdb.yandexcloud.net`
# - **База данных** — `data-analyst-afisha`
# - **Порт** — `6432`
# - **Аутентификация** — `Database Native`
# - **Пользователь** — `praktikum_student`
# - **Пароль** — `Sdf4$2;d-d30pp`
# 
# Для выгрузки используйте запрос из предыдущего урока и библиотеку SQLAlchemy.
# 
# Выгрузка из базы данных SQL должна позволить собрать следующие данные:
# 
# - `user_id` — уникальный идентификатор пользователя, совершившего заказ;
# - `device_type_canonical` — тип устройства, с которого был оформлен заказ (`mobile` — мобильные устройства, `desktop` — стационарные);
# - `order_id` — уникальный идентификатор заказа;
# - `order_dt` — дата создания заказа (используйте данные `created_dt_msk`);
# - `order_ts` — дата и время создания заказа (используйте данные `created_ts_msk`);
# - `currency_code` — валюта оплаты;
# - `revenue` — выручка от заказа;
# - `tickets_count` — количество купленных билетов;
# - `days_since_prev` — количество дней от предыдущей покупки пользователя, для пользователей с одной покупкой — значение пропущено;
# - `event_id` — уникальный идентификатор мероприятия;
# - `service_name` — название билетного оператора;
# - `event_type_main` — основной тип мероприятия (театральная постановка, концерт и так далее);
# - `region_name` — название региона, в котором прошло мероприятие;
# - `city_name` — название города, в котором прошло мероприятие.
# 
# ---
# 

# In[1]:


# Используйте ячейки типа Code для вашего кода,
# а ячейки типа Markdown для комментариев и выводов


# In[2]:


# При необходимости добавляйте новые ячейки для кода или текста


# In[3]:


get_ipython().system('pip install sqlalchemy -q')
get_ipython().system('pip install psycopg2-binary -q')
get_ipython().system('pip install phik -q')


# In[4]:


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from phik import phik_matrix
from sqlalchemy import create_engine


# In[5]:


db_config = {'user': 'praktikum_student', # имя пользователя
             'pwd': 'Sdf4$2;d-d30pp', # пароль
             'host': 'rc1b-wcoijxj3yxfsf3fs.mdb.yandexcloud.net',
             'port': 6432, # порт подключения
             'db': 'data-analyst-afisha' # название базы данных
             }


# In[6]:


connection_string = 'postgresql://{}:{}@{}:{}/{}'.format(
    db_config['user'],
    db_config['pwd'],
    db_config['host'],
    db_config['port'],
    db_config['db'],
)


# In[7]:


engine = create_engine(connection_string)


# In[8]:


query = '''
WITH set_config_precode AS (
  SELECT set_config('synchronize_seqscans', 'off', true)
)
SELECT 
  p.user_id,
  p.device_type_canonical,
  p.order_id,
  p.created_dt_msk as order_dt,
  p.created_ts_msk as order_ts,
  p.currency_code,
  p.revenue,
  p.tickets_count,
  p.created_dt_msk::date - 
  LAG(p.created_dt_msk::date) OVER(PARTITION BY p.user_id ORDER BY p.created_dt_msk)
  AS days_since_prev,
  p.event_id,
  e.event_name_code as event_name,
  e.event_type_main,
  p.service_name,
  r.region_name,
  c.city_name
FROM afisha.purchases as p 
JOIN afisha.events AS e using(event_id)
JOIN afisha.city as c using(city_id)
JOIN afisha.regions as r using(region_id)
WHERE p.device_type_canonical IN ('mobile', 'desktop') and event_type_main != 'фильм'
ORDER BY p.user_id
'''


# In[9]:


df = pd.read_sql_query(query, con=engine)


# ---
# 
# **Задача 1.2:** Изучите общую информацию о выгруженных данных. Оцените корректность выгрузки и объём полученных данных.
# 
# Предположите, какие шаги необходимо сделать на стадии предобработки данных — например, скорректировать типы данных.
# 
# Зафиксируйте основную информацию о данных в кратком промежуточном выводе.
# 
# ---

# In[10]:


display(df.info())


# In[11]:


display(df.head())


# В датафрейме имеется 15 столбцов и 290611 строк. Он занимает 33.3+ MB. Пропуски имеются в столбце days_since_prev, они связанны с тем, что для пользователей с одной покупкой — значение пропущено.
# 
# Думаю стоит заменить пропущенные значения для пользователей с одним заказом, на большое число, например 10000. Удалять данные строки нельзя, т.к. они являются конеретной группой пользователей, которые мы изучаем. 
# 
# Возможно стоит сменить размерность столбца tickets_count на более меньшую. 

# ---
# 
# ###  2. Предобработка данных
# 
# Выполните все стандартные действия по предобработке данных:
# 
# ---
# 
# **Задача 2.1:** Данные о выручке сервиса представлены в российских рублях и казахстанских тенге. Приведите выручку к единой валюте — российскому рублю.
# 
# Для этого используйте датасет с информацией о курсе казахстанского тенге по отношению к российскому рублю за 2024 год — `final_tickets_tenge_df.csv`. Его можно загрузить по пути `https://code.s3.yandex.net/datasets/final_tickets_tenge_df.csv')`
# 
# Значения в рублях представлено для 100 тенге.
# 
# Результаты преобразования сохраните в новый столбец `revenue_rub`.
# 
# ---
# 

# In[12]:


df_tenge = pd.read_csv('https://code.s3.yandex.net/datasets/final_tickets_tenge_df.csv')


# In[13]:


display(df_tenge.info())


# In[14]:


display(df_tenge.head(5))


# In[15]:


df_tenge['data'] = pd.to_datetime(df_tenge['data'], format = '%Y-%m-%d')


# In[16]:


merged_df = pd.merge(df, df_tenge, left_on='order_dt', right_on='data')


# In[17]:


merged_df['revenue_rub'] = merged_df.apply(lambda row: row['revenue'] * row['curs'] / 100 if row['currency_code'] == 'kzt' else row['revenue'], axis=1)


# In[18]:


kzt_rows = merged_df.loc[merged_df['currency_code'] == 'kzt']
display(kzt_rows)


# In[19]:


df = merged_df.drop(['data', 'nominal', 'curs', 'cdx'], axis=1)


# In[20]:


display(df.info())


# In[21]:


display(df.head(5))


# Были соединены два датасета df и df_tenge, выручка в тенге была приведена к рублю в новом столбце revenue_rub

# ---
# 
# **Задача 2.2:**
# 
# - Проверьте данные на пропущенные значения. Если выгрузка из SQL была успешной, то пропуски должны быть только в столбце `days_since_prev`.
# - Преобразуйте типы данных в некоторых столбцах, если это необходимо. Обратите внимание на данные с датой и временем, а также на числовые данные, размерность которых можно сократить.
# - Изучите значения в ключевых столбцах. Обработайте ошибки, если обнаружите их.
#     - Проверьте, какие категории указаны в столбцах с номинальными данными. Есть ли среди категорий такие, что обозначают пропуски в данных или отсутствие информации? Проведите нормализацию данных, если это необходимо.
#     - Проверьте распределение численных данных и наличие в них выбросов. Для этого используйте статистические показатели, гистограммы распределения значений или диаграммы размаха.
#         
#         Важные показатели в рамках поставленной задачи — это выручка с заказа (`revenue_rub`) и количество билетов в заказе (`tickets_count`), поэтому в первую очередь проверьте данные в этих столбцах.
#         
#         Если обнаружите выбросы в поле `revenue_rub`, то отфильтруйте значения по 99 перцентилю.
# 
# После предобработки проверьте, были ли отфильтрованы данные. Если были, то оцените, в каком объёме. Сформулируйте промежуточный вывод, зафиксировав основные действия и описания новых столбцов.
# 
# ---

# In[22]:


missing_values = df.isnull().sum()
display(missing_values)


# In[23]:


missing_values_per = df.isnull().sum() / df.shape[0] * 100
display(missing_values_per)


# In[24]:


def optimize_memory_usage(df: pd.DataFrame, print_size: bool=True) -> pd.DataFrame:
    '''
    Function optimizes memory usage in dataframe
    df: pd.DataFrame - data table
    print_size: bool - display of optimization results
    return pd.DataFrame - amount of optimized memory
    '''
    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64'] # Типы, которые будем проверять на оптимизацию
    # Размер занимаемой памяти до оптимизации (в Мб)
    before_size = df.memory_usage().sum() / 1024**2    
    for column in df.columns:
        column_type = df[column].dtypes
        if column_type in numerics:
            column_min = df[column].min()
            column_max = df[column].max()
            if str(column_type).startswith('int'):
                if column_min > np.iinfo(np.int8).min and column_max < np.iinfo(np.int8).max:
                    df[column] = df[column].astype(np.int8)
                elif column_min > np.iinfo(np.int16).min and column_max < np.iinfo(np.int16).max:
                    df[column] = df[column].astype(np.int16)
                elif column_min > np.iinfo(np.int32).min and column_max < np.iinfo(np.int32).max:
                    df[column] = df[column].astype(np.int32)
                elif column_min > np.iinfo(np.int64).min and column_max < np.iinfo(np.int64).max:
                    df[column] = df[column].astype(np.int64)  
            else:
                if column_min > np.finfo(np.float32).min and column_max < np.finfo(np.float32).max:
                    df[column] = df[column].astype(np.float32)
                else:
                    df[column] = df[column].astype(np.float64)    
    # Размер занимаемой памяти после оптимизации (в Мб)
    after_size = df.memory_usage().sum() / 1024**2
    if print_size: print('Размер использования памяти: до {:5.2f} Mb - после {:5.2f} Mb ({:.1f}%)'
                         .format(before_size, after_size, 100 * (before_size - after_size) / before_size))
    return df


# In[25]:


optimize_memory_usage(df, True)


# In[26]:


display(df.info())


# In[27]:


print(df['tickets_count'].describe())


# In[28]:


Q1tc = df['tickets_count'].quantile(0.25)
Q3tc = df['tickets_count'].quantile(0.75)
IQR = Q3tc - Q1tc
lower_bound = Q1tc - 2 * IQR
upper_bound = Q3tc + 2 * IQR
outliers = df[(df['tickets_count'] < lower_bound) | (df['tickets_count'] > upper_bound)]
display(outliers)


# In[29]:


plt.figure(figsize=(10, 6))
sns.boxplot(x='tickets_count', data=df, orient='h')
plt.title('Диаграмма размаха количества билетов в заказе')
plt.xlabel('Количество билетов в заказе')
plt.xlim(left=0, right=df['tickets_count'].quantile(0.99))

plt.show()


# In[30]:


print(df['revenue_rub'].describe())


# In[31]:


plt.figure(figsize=(10, 6))
sns.boxplot(x='revenue_rub', data=df, orient='h')
plt.title('Диаграмма размаха выручки в рублях')
plt.xlabel('Количество рублей')
plt.xlim(left=0, right=df['revenue_rub'].quantile(0.85))

plt.show()


# In[32]:


not_filtered_count = df.shape[0]
threshold = df['revenue_rub'].quantile(0.99)
filtered_df = df[df['revenue_rub'] <= threshold]
filtered_count = filtered_df.shape[0]

display(not_filtered_count - filtered_count)
display((not_filtered_count - filtered_count) / (not_filtered_count) * 100)


# В датасете пропущенные значения есть в days_since_prev(21933), они составляют около 8% от всех данных.
# Была проведена оптимизация типов данных.
# 
# Были проанализированны столбцы revenue_rub и tickets_count, построины диаграммы размаха. Были удалены строки где revenue_rub выходили за 99 процентиль. Всего удалено 2825 строк это меньше 1%.
# 
# В столбце revenue_rub есть отрицательные значения, это может быть связанно с тем, что билеты продавали ниже себе стоимости.
# Также стоит отметить максимальное кол-во билетов в одном заказе 57.

# ---
# 
# ### 3. Создание профиля пользователя
# 
# В будущем отдел маркетинга планирует создать модель для прогнозирования возврата пользователей. Поэтому сейчас они просят вас построить агрегированные признаки, описывающие поведение и профиль каждого пользователя.
# 
# ---
# 
# **Задача 3.1.** Постройте профиль пользователя — для каждого пользователя найдите:
# 
# - дату первого и последнего заказа;
# - устройство, с которого был сделан первый заказ;
# - регион, в котором был сделан первый заказ;
# - билетного партнёра, к которому обращались при первом заказе;
# - жанр первого посещённого мероприятия (используйте поле `event_type_main`);
# - общее количество заказов;
# - средняя выручка с одного заказа в рублях;
# - среднее количество билетов в заказе;
# - среднее время между заказами.
# 
# После этого добавьте два бинарных признака:
# 
# - `is_two` — совершил ли пользователь 2 и более заказа;
# - `is_five` — совершил ли пользователь 5 и более заказов.
# 
# **Рекомендация:** перед тем как строить профиль, отсортируйте данные по времени совершения заказа.
# 
# ---
# 

# In[33]:


sort_df = filtered_df.sort_values(by = 'order_dt')


# In[34]:


group_df = sort_df.groupby('user_id').agg(
    first_order_date=('order_dt', 'first'),
    last_order_date=('order_dt', 'last'),
    first_device_type=('device_type_canonical','first'),
    first_region = ('region_name', 'first'),
    first_service_name = ('service_name', 'first'),
    first_event_type_main = ('event_type_main', 'first'),
    total_orders = ('order_id','count'),
    avg_revenue = ('revenue_rub','mean'),
    avg_tickets = ('tickets_count','mean'),
    avg_days_since_prev = ('days_since_prev', 'mean'),
     )


# In[35]:


group_df['is_two'] = group_df['total_orders'].apply(lambda x: 1 if (x >= 2) else 0)
group_df['is_five'] = group_df['total_orders'].apply(lambda x: 1 if x >= 5 else 0)


# In[36]:


display(group_df)


# Был создан датафрейм group_df с профилями пользователей

# ---
# 
# **Задача 3.2.** Прежде чем проводить исследовательский анализ данных и делать выводы, важно понять, с какими данными вы работаете: насколько они репрезентативны и нет ли в них аномалий.
# 
# Используя данные о профилях пользователей, рассчитайте:
# 
# - общее число пользователей в выборке;
# - среднюю выручку с одного заказа;
# - долю пользователей, совершивших 2 и более заказа;
# - долю пользователей, совершивших 5 и более заказов.
# 
# Также изучите статистические показатели:
# 
# - по общему числу заказов;
# - по среднему числу билетов в заказе;
# - по среднему количеству дней между покупками.
# 
# По результатам оцените данные: достаточно ли их по объёму, есть ли аномальные значения в данных о количестве заказов и среднем количестве билетов?
# 
# Если вы найдёте аномальные значения, опишите их и примите обоснованное решение о том, как с ними поступить:
# 
# - Оставить и учитывать их при анализе?
# - Отфильтровать данные по какому-то значению, например, по 95-му или 99-му перцентилю?
# 
# Если вы проведёте фильтрацию, то вычислите объём отфильтрованных данных и выведите статистические показатели по обновлённому датасету.

# In[37]:


display(group_df.shape[0])


# In[38]:


display(group_df['avg_revenue'].mean())


# In[39]:


is_two_df  = group_df[group_df['is_two'] == 1]
display(is_two_df.shape[0])
display(is_two_df.shape[0] / group_df.shape[0] * 100)


# In[40]:


is_five_df = group_df[group_df['is_five'] == 1]
display(is_five_df.shape[0])
display(is_five_df.shape[0] / group_df.shape[0] * 100)


# In[41]:


display(group_df['total_orders'].describe())


# In[42]:


display(group_df['total_orders'].sum())


# In[43]:


plt.figure(figsize = (10, 6))
sns.barplot(x = 'total_orders', data = group_df)
plt.title('Диаграмма размаха количества заказов')
plt.xlabel('Количество заказов')
plt.xlim(left= -1, right=group_df['total_orders'].quantile(0.90))
plt.show()


# In[44]:


display(group_df['avg_tickets'].describe())


# In[45]:


display(group_df['avg_days_since_prev'].describe())


# In[46]:


not_filtered_group_df = group_df.shape[0]
threshold = group_df['total_orders'].quantile(0.99)
filtered_group_df = group_df[group_df['total_orders'] <= threshold]
filtered_group_df = filtered_group_df.reset_index()
filtered_group_count = filtered_group_df.shape[0]
display(not_filtered_group_df)
display(not_filtered_group_df - filtered_group_count)
display((not_filtered_group_df - filtered_group_count) / not_filtered_group_df * 100)


# In[47]:


display(filtered_group_df.shape[0])


# In[48]:


display(filtered_group_df['avg_revenue'].mean())


# In[49]:


is_two_df  = filtered_group_df[filtered_group_df['is_two'] == 1]
display(is_two_df.shape[0])
display(is_two_df.shape[0] / filtered_group_df.shape[0] * 100)


# In[50]:


is_five_df = filtered_group_df[filtered_group_df['is_five'] == 1]
display(is_five_df.shape[0])
display(is_five_df.shape[0] / filtered_group_df.shape[0] * 100)


# In[51]:


display(filtered_group_df['total_orders'].describe())


# In[52]:


display(filtered_group_df['total_orders'].sum())


# In[53]:


plt.figure(figsize = (10, 6))
sns.barplot(x = 'total_orders', data = filtered_group_df)
plt.title('Диаграмма размаха количества заказов')
plt.xlabel('Количество заказов')
plt.xlim(left= -1, right=group_df['total_orders'].quantile(0.80))
plt.show()


# In[54]:


display(filtered_group_df['avg_tickets'].describe())


# In[55]:


display(filtered_group_df['avg_days_since_prev'].describe())


# Всего пользователе 21854, средняя выручка с одного заказа 544.3981, доля пользователей, совершивших 2 и более заказа 61.709526860071385, доля пользователей, совершивших 5 и более заказов 29.00613160062231. 
# 
# Общее число заказов 287786, что достаточно много для такого количества пользователей, это может означать о наличии аномалий в данных. Максимальное количество билетов у одного пользователя 10181, это слишком много для периода времени который мы рассматриваем. Стоит удалить данные которе превышают 99 процентиль. 
# 
# Статистичесие значения у столбцов avg_tickets и avg_days_since_prev нормальные, но стоит заметить что avg_days_since_prev вошли только 13513 записей, т.к. у пользователей с 1 покупкой стоит NaN. 
# 
# Были удалены 216 записей с аномальными данными, они составляли около 1% от всех данных. 
# После удаления данных, всего пользователе 21638, средняя выручка с одного заказа 544.7078, доля пользователей, совершивших 2 и более заказа 61.327294574359925, доля пользователей, совершивших 5 и более заказов 28.297439689435254. 
# 
# Общее число заказов 140675, гараздо меньше. Максимальное количество билетов у одного пользователя 152, что возможно. 
# 
# Статистичесие значения у столбцов avg_tickets и avg_days_since_prev не сильно изменились.

# ---
# 
# ### 4. Исследовательский анализ данных
# 
# Следующий этап — исследование признаков, влияющих на возврат пользователей, то есть на совершение повторного заказа. Для этого используйте профили пользователей.

# 
# 
# #### 4.1. Исследование признаков первого заказа и их связи с возвращением на платформу
# 
# Исследуйте признаки, описывающие первый заказ пользователя, и выясните, влияют ли они на вероятность возвращения пользователя.
# 
# ---
# 
# **Задача 4.1.1.** Изучите распределение пользователей по признакам.
# 
# - Сгруппируйте пользователей:
#     - по типу их первого мероприятия;
#     - по типу устройства, с которого совершена первая покупка;
#     - по региону проведения мероприятия из первого заказа;
#     - по билетному оператору, продавшему билеты на первый заказ.
# - Подсчитайте общее количество пользователей в каждом сегменте и их долю в разрезе каждого признака. Сегмент — это группа пользователей, объединённых определённым признаком, то есть объединённые принадлежностью к категории. Например, все клиенты, сделавшие первый заказ с мобильного телефона, — это сегмент.
# - Ответьте на вопрос: равномерно ли распределены пользователи по сегментам или есть выраженные «точки входа» — сегменты с наибольшим числом пользователей?
# 
# ---
# 

# In[56]:


display(filtered_group_df['first_event_type_main'].value_counts())


# In[57]:


filtered_group_df['first_event_type_main'].value_counts(normalize=True).plot(kind = 'bar', title = 'Количество пользователей как первое мероприятия',
                ylabel='Количество пользователей',
                xlabel='Мероприятия',
                rot=0,
                figsize=(10, 6),
                )
plt.show()


# In[58]:


display(filtered_group_df['first_device_type'].value_counts())


# In[59]:


filtered_group_df['first_device_type'].value_counts(normalize=True).plot(kind = 'bar', title = 'Количество пользователей по каждому устройству',
                ylabel='Количество пользователей',
                xlabel='Типы устройств',
                rot=0,
                figsize=(10, 6),
                )
plt.show()


# In[60]:


display(filtered_group_df['first_region'].value_counts())


# In[61]:


filtered_group_df['first_region'].value_counts(normalize=True).plot(kind = 'barh', title = 'Количество пользователей в каждом регионе',
                ylabel='Количество пользователей',
                xlabel='Регионы',
                rot=0                                                            ,
                figsize=(15, 15),
                )
plt.show()


# In[62]:


display(filtered_group_df['first_service_name'].value_counts())


# In[63]:


filtered_group_df['first_service_name'].value_counts(normalize=True).plot(kind = 'barh', title = 'Количество пользователей по сервисам',
                ylabel='Количество пользователей',
                xlabel='Сервисы',
                rot=0                                                            ,
                figsize=(15, 10),
                )
plt.show()


# Во всех случаях пользователи распределенны не равномерно. Чаще всего люди идут на концерты, как первое мероприятие 40%. Больше всего пользователей, кто сделал первый заказ с телефона около 80%. Больше всего первых заказов было сделано из Каменевского региона, около трети пользователей. Самый частый сервис Билеты без проблем, чуть мень четверти. 

# ---
# 
# **Задача 4.1.2.** Проанализируйте возвраты пользователей:
# 
# - Для каждого сегмента вычислите долю пользователей, совершивших два и более заказа.
# - Визуализируйте результат подходящим графиком. Если сегментов слишком много, то поместите на график только 10 сегментов с наибольшим количеством пользователей. Такое возможно с сегментами по региону и по билетному оператору.
# - Ответьте на вопросы:
#     - Какие сегменты пользователей чаще возвращаются на Яндекс Афишу?
#     - Наблюдаются ли успешные «точки входа» — такие сегменты, в которых пользователи чаще совершают повторный заказ, чем в среднем по выборке?
# 
# При интерпретации результатов учитывайте размер сегментов: если в сегменте мало пользователей (например, десятки), то доли могут быть нестабильными и недостоверными, то есть показывать широкую вариацию значений.
# 
# ---
# 

# In[64]:


filtered_group_df.groupby('first_event_type_main')['is_two'].mean().plot(kind = 'bar', title = 'Доля пользователей, совершивших 2 и более заказа',
                ylabel='Доля пользователей',
                xlabel='',
                rot=0,
                figsize=(15, 10),
                )
plt.show()


# In[65]:


filtered_group_df.groupby('first_device_type')['is_two'].mean().plot(kind = 'bar', title = 'Доля пользователей, совершивших 2 и более заказа',
                ylabel='Доля пользователей',
                xlabel='Типы устройств',
                rot=0                                                            ,
                figsize=(15, 10),
                )
plt.show()


# In[66]:


grouped_df = filtered_group_df.groupby('first_region')['user_id'].count().reset_index()
sorted_df = grouped_df.sort_values(by='user_id', ascending=False).head(10)
filtered_group_df[filtered_group_df['first_region'].isin(sorted_df['first_region'])].groupby('first_region')['is_two'].mean().plot(kind = 'barh', title = 'Доля пользователей, совершивших 2 и более заказа',
                ylabel='Доля пользователей',
                xlabel='Регионы',
                rot=0                                                            ,
                figsize=(15, 10),
                )
plt.show()



# In[67]:


grouped_df = filtered_group_df.groupby('first_service_name')['user_id'].count().reset_index()
sorted_df = grouped_df.sort_values(by='user_id', ascending=False).head(10)
filtered_group_df[filtered_group_df['first_service_name'].isin(sorted_df['first_service_name'])].groupby('first_service_name')['is_two'].mean().plot(kind = 'barh', title = 'Доля пользователей, совершивших 2 и более заказа',
                ylabel='Доля пользователей',
                xlabel='Сервисы',
                rot=0                                                            ,
                figsize=(15, 10),
                )
plt.show()


# Из мероприятий чаще всего люди возвращаются на выстовки, театры и концерты, но стоит подметить что выстовки и театры имеют малую долю относительно всех мероприятий.
# 
# Люди кто сделал первых заказ через desctop возвращаются чуть чаще чем mobile, но заказов с mobile крайне больше. 
# 
# Люди кто сделал первых заказ из Шанырского региона и Светополянский округа возвращаются чаще, что касается Каменевского региона и Североярской области, которые являются топ-1 и топ-2 по кол-ву пользователей, то они имеют меньшую долю.
# 
# Среди первых сервисов чаще всего возвращаются из Край Билетов и Дом культуры, что касается Билеты без проблем и Мой билет, 
# которые являются топ-1 и топ-2 по кол-ву пользователей, их доля меньше.

# ---
# 
# **Задача 4.1.3.** Опираясь на выводы из задач выше, проверьте продуктовые гипотезы:
# 
# - **Гипотеза 1.** Тип мероприятия влияет на вероятность возврата на Яндекс Афишу: пользователи, которые совершили первый заказ на спортивные мероприятия, совершают повторный заказ чаще, чем пользователи, оформившие свой первый заказ на концерты.
# - **Гипотеза 2.** В регионах, где больше всего пользователей посещают мероприятия, выше доля повторных заказов, чем в менее активных регионах.
# 
# ---

# Гипотеза 1. Нет, пользователи совершившие первый заказ на концерты возвращаются чаще, чем пользователи совершившие первый заказна спортивные мероприятия.

# Гипотеза 2. Нет, как было написанно выще, люди из Каменевского региона и Североярской области совершают меньше повторных заказов, чем из Шанырского региона и Светополянский округа.
# 

# ---
# 
# #### 4.2. Исследование поведения пользователей через показатели выручки и состава заказа
# 
# Изучите количественные характеристики заказов пользователей, чтобы узнать среднюю выручку сервиса с заказа и количество билетов, которое пользователи обычно покупают.
# 
# Эти метрики важны не только для оценки выручки, но и для оценки вовлечённости пользователей. Возможно, пользователи с более крупными и дорогими заказами более заинтересованы в сервисе и поэтому чаще возвращаются.
# 
# ---
# 
# **Задача 4.2.1.** Проследите связь между средней выручкой сервиса с заказа и повторными заказами.
# 
# - Постройте сравнительные гистограммы распределения средней выручки с билета (`avg_revenue_rub`):
#     - для пользователей, совершивших один заказ;
#     - для вернувшихся пользователей, совершивших 2 и более заказа.
# - Ответьте на вопросы:
#     - В каких диапазонах средней выручки концентрируются пользователи из каждой группы?
#     - Есть ли различия между группами?
# 
# Текст на сером фоне:
#     
# **Рекомендация:**
# 
# 1. Используйте одинаковые интервалы (`bins`) и прозрачность (`alpha`), чтобы визуально сопоставить распределения.
# 2. Задайте параметру `density` значение `True`, чтобы сравнивать форму распределений, даже если число пользователей в группах отличается.
# 
# ---
# 

# In[68]:


plt.figure(figsize = (15,10))
sns.histplot(data = filtered_group_df, hue = 'is_two', bins = 50, alpha = 0.5, x = 'avg_revenue', stat='density')
plt.title('График распределения средней выручки для пользователей')
plt.xlabel('Средняя выручка')
plt.legend(labels=['Два и более заказов', 'Меньше двух заказов'])
plt.xticks(range(0, int(filtered_group_df['avg_revenue'].max()) + 100, 100)) 
plt.show()


# У пользователей с двумя и более заказами средняя выручка варируется от 350 до 750 руб., так же есть большая группа в 100 руб.
# У пользователей с одним заказом средняя выручка варируется от 0 до 100 руб.
# 
# Также стоит отметить, что существуют заказы с отрицательной выручкой(скидки/промоакции), это может влиять.
# 

# ---
# 
# **Задача 4.2.2.** Сравните распределение по средней выручке с заказа в двух группах пользователей:
# 
# - совершившие 2–4 заказа;
# - совершившие 5 и более заказов.
# 
# Ответьте на вопрос: есть ли различия по значению средней выручки с заказа между пользователями этих двух групп?
# 
# ---
# 

# In[69]:


orders_2_4 = filtered_group_df[(filtered_group_df['total_orders']>=2) & (filtered_group_df['total_orders'] <=4)]


# In[70]:


orders_5 = filtered_group_df[filtered_group_df['total_orders']>=5]


# In[71]:


plt.figure(figsize = (15,10))
sns.histplot(data = orders_2_4,  bins = 50, alpha = 0.5, x = 'avg_revenue', stat='density')
plt.title('График распределения средней выручки для пользователей от 2 до 4 заказов')
plt.xlabel('Средняя выручка')
plt.xticks(range(0, int(filtered_group_df['avg_revenue'].max()) + 100, 100)) 
plt.show()


# In[72]:


plt.figure(figsize = (15,10))
sns.histplot(data = orders_5,  bins = 50, alpha = 0.5, x = 'avg_revenue', stat='density')
plt.title('График распределения средней выручки для пользователей с 5 и более заказами')
plt.xlabel('Средняя выручка')
plt.xticks(range(0, int(filtered_group_df['avg_revenue'].max()) + 100, 100)) 
plt.show()


# Различия между группами есть. У пользователей с количеством заказов от двух до четырех средняя выручка варируется от 0 до 100, также выделяется группа 200. У пользователей с пятью и более заказами средняя выручка варируется от 350 до 750 руб.
# 
# 

# ---
# 
# **Задача 4.2.3.** Проанализируйте влияние среднего количества билетов в заказе на вероятность повторной покупки.
# 
# - Изучите распределение пользователей по среднему количеству билетов в заказе (`avg_tickets_count`) и опишите основные наблюдения.
# - Разделите пользователей на несколько сегментов по среднему количеству билетов в заказе:
#     - от 1 до 2 билетов;
#     - от 2 до 3 билетов;
#     - от 3 до 5 билетов;
#     - от 5 и более билетов.
# - Для каждого сегмента подсчитайте общее число пользователей и долю пользователей, совершивших повторные заказы.
# - Ответьте на вопросы:
#     - Как распределены пользователи по сегментам — равномерно или сконцентрировано?
#     - Есть ли сегменты с аномально высокой или низкой долей повторных покупок?
# 
# ---

# In[73]:


plt.figure(figsize = (15,10))
sns.histplot(data = filtered_group_df,  bins = 50, alpha = 0.5, x = 'avg_tickets')
plt.title('График распределения пользователей по среднему количеству билетов в заказе ')
plt.xlabel('Среднее кол-во билетов')
plt.ylabel('Количество')
plt.xticks(range(0, int(filtered_group_df['avg_tickets'].max()) + 1, 1))
plt.show()


# In[74]:


tickets_1_2 = filtered_group_df[(filtered_group_df['avg_tickets']>=1) & (filtered_group_df['avg_tickets'] < 2)]
tickets_2_3 = filtered_group_df[(filtered_group_df['avg_tickets']>=2) & (filtered_group_df['avg_tickets'] < 3)]
tickets_3_5 = filtered_group_df[(filtered_group_df['avg_tickets']>=3) & (filtered_group_df['avg_tickets'] < 5)]
tickets_5 = filtered_group_df[(filtered_group_df['avg_tickets']>=5)]


# In[75]:


display(tickets_1_2['user_id'].count())
display(tickets_1_2['is_two'].mean())


# In[76]:


display(tickets_2_3['user_id'].count())
display(tickets_2_3['is_two'].mean())


# In[77]:


display(tickets_3_5['user_id'].count())
display(tickets_3_5['is_two'].mean())


# In[78]:


display(tickets_5['user_id'].count())
display(tickets_5['is_two'].mean())


# От 1 до 2 и от 3 до 5 распределены равномерно (0.5130219098801158, 0.5426048565121413).
# От 2 до 3 чаще возвращаются на платформу(0.7358391240261107).
# От 5 и более мегьше всего(0.1875945537065053). Стоит обратить снимание на эту группу пользователей.

# ---
# 
# #### 4.3. Исследование временных характеристик первого заказа и их влияния на повторные покупки
# 
# Изучите временные параметры, связанные с первым заказом пользователей:
# 
# - день недели первой покупки;
# - время с момента первой покупки — лайфтайм;
# - средний интервал между покупками пользователей с повторными заказами.
# 
# ---
# 
# **Задача 4.3.1.** Проанализируйте, как день недели, в которой была совершена первая покупка, влияет на поведение пользователей.
# 
# - По данным даты первого заказа выделите день недели.
# - Для каждого дня недели подсчитайте общее число пользователей и долю пользователей, совершивших повторные заказы. Результаты визуализируйте.
# - Ответьте на вопрос: влияет ли день недели, в которую совершена первая покупка, на вероятность возврата клиента?
# 
# ---
# 

# In[79]:


filtered_group_df['week_day'] = filtered_group_df['first_order_date'].dt.day_name()


# In[80]:


display(filtered_group_df.groupby('week_day')['user_id'].count())


# In[81]:


filtered_group_df.groupby('week_day')['is_two'].mean().plot(kind = 'bar', title = 'Доля пользователей, совершивших 2 и более заказа',
                ylabel='Доля пользователей',
                xlabel='Дни недели',
                rot=0                                                            ,
                figsize=(15, 10),
                )
plt.show()


# День недели, в который совершена первая покупка, не влияет на  вероятность возврата клиента

# ---
# 
# **Задача 4.3.2.** Изучите, как средний интервал между заказами влияет на удержание клиентов.
# 
# - Рассчитайте среднее время между заказами для двух групп пользователей:
#     - совершившие 2–4 заказа;
#     - совершившие 5 и более заказов.
# - Исследуйте, как средний интервал между заказами влияет на вероятность повторного заказа, и сделайте выводы.
# 
# ---
# 

# In[82]:


display(orders_2_4['avg_days_since_prev'].mean())


# In[83]:


display(orders_5['avg_days_since_prev'].mean())


# У пользователей которые совершили 5 и более заказов интервал между заказами меньше, чем у пользователей совершивших от 2 до 4 заказов (9.900063 < 21.329939). Это может указывать на то, что более частые заказы (меньший интервал) способствуют увеличению вероятности повторного заказа. Пользователи, делающие заказы чаще, склонны совершать больше заказов в целом.

# ---
# 
# #### 4.4. Корреляционный анализ количества покупок и признаков пользователя
# 
# Изучите, какие характеристики первого заказа и профиля пользователя могут быть связаны с числом покупок. Для этого используйте универсальный коэффициент корреляции `phi_k`, который позволяет анализировать как числовые, так и категориальные признаки.
# 
# ---
# 
# **Задача 4.4.1:** Проведите корреляционный анализ:
# - Рассчитайте коэффициент корреляции `phi_k` между признаками профиля пользователя и числом заказов (`total_orders`). При необходимости используйте параметр `interval_cols` для определения интервальных данных.
# - Проанализируйте полученные результаты. Если полученные значения будут близки к нулю, проверьте разброс данных в `total_orders`. Такое возможно, когда в данных преобладает одно значение: в таком случае корреляционный анализ может показать отсутствие связей. Чтобы этого избежать, выделите сегменты пользователей по полю `total_orders`, а затем повторите корреляционный анализ. Выделите такие сегменты:
#     - 1 заказ;
#     - от 2 до 4 заказов;
#     - от 5 и выше.
# - Визуализируйте результат корреляции с помощью тепловой карты.
# - Ответьте на вопрос: какие признаки наиболее связаны с количеством заказов?
# 
# ---

# In[86]:


orders_1 = filtered_group_df[filtered_group_df['total_orders'] == 1]


# In[90]:


col_matrix = filtered_group_df.phik_matrix(interval_cols=['total_orders', 'avg_revenue', 'avg_tickets', 'avg_days_since_prev'])


# In[91]:


plt.figure(figsize=(10, 6))
sns.heatmap(col_matrix, annot=True, cmap='coolwarm')
plt.title('Матрица корреляции')
plt.show()


# In[93]:


display(col_matrix['total_orders'].sort_values(ascending = False))


# Самые сильные признаки связанные с total_orders это is_five(0.615266),first_order_date(0.429065),last_order_date(0.410308), is_two(0.309085).
# is_five имеет заметную корреляцию.
# first_order_date, last_order_date, is_two имеют умеренную корреляцию.

# ### 5. Общий вывод и рекомендации
# 
# В конце проекта напишите общий вывод и рекомендации: расскажите заказчику, на что нужно обратить внимание. В выводах кратко укажите:
# 
# - **Информацию о данных**, с которыми вы работали, и то, как они были подготовлены: например, расскажите о фильтрации данных, переводе тенге в рубли, фильтрации выбросов.
# - **Основные результаты анализа.** Например, укажите:
#     - Сколько пользователей в выборке? Как распределены пользователи по числу заказов? Какие ещё статистические показатели вы подсчитали важным во время изучения данных?
#     - Какие признаки первого заказа связаны с возвратом пользователей?
#     - Как связаны средняя выручка и количество билетов в заказе с вероятностью повторных покупок?
#     - Какие временные характеристики влияют на удержание (день недели, интервалы между покупками)?
#     - Какие характеристики первого заказа и профиля пользователя могут быть связаны с числом покупок согласно результатам корреляционного анализа?
# - Дополните выводы информацией, которая покажется вам важной и интересной. Следите за общим объёмом выводов — они должны быть компактными и ёмкими.
# 
# В конце предложите заказчику рекомендации о том, как именно действовать в его ситуации. Например, укажите, на какие сегменты пользователей стоит обратить внимание в первую очередь, а какие нуждаются в дополнительных маркетинговых усилиях.

# В ходе выполнения анализа средняя выручка была преведена в рубли. Были удалены строки где revenue_rub выходили за 99 процентиль.Всего удалено 2825 строк это меньше 1%.
# 
# Всего пользователей 21638, пользователей совершивших 2 и более заказа 61.327294574359925, доля пользователей, совершивших 5 и более заказов 28.297439689435254, общее число заказов 140675, средняя выручка с одного заказа 544.7078.
# 
# Из мероприятий чаще всего люди возвращаются на выстовки, театры и концерты, но стоит подметить что выстовки и театры имеют малую долю относительно всех мероприятий.
# Люди кто сделал первых заказ через desctop возвращаются чуть чаще чем mobile, но заказов с mobile крайне больше.
# Люди кто сделал первых заказ из Шанырского региона и Светополянский округа возвращаются чаще, что касается Каменевского региона и Североярской области, которые являются топ-1 и топ-2 по кол-ву пользователей, то они имеют меньшую долю.
# Среди первых сервисов чаще всего возвращаются из Край Билетов и Дом культуры, что касается Билеты без проблем и Мой билет, которые являются топ-1 и топ-2 по кол-ву пользователей, их доля меньше.
# 
# У пользователей с двумя и более заказами средняя выручка варируется от 350 до 750 руб., так же есть большая группа в 100 руб. У пользователей с одним заказом средняя выручка варируется от 0 до 100 руб.
# Также стоит отметить, что существуют заказы с отрицательной выручкой(скидки/промоакции), это может влиять.
# У пользователей с количеством заказов от двух до четырех средняя выручка варируется от 0 до 100, также выделяется группа 200. У пользователей с пятью и более заказами средняя выручка варируется от 350 до 750 руб.
# 
# День недели, в который совершена первая покупка, не влияет на вероятность возврата клиента. У пользователей которые совершили 5 и более заказов интервал между заказами меньше, чем у пользователей совершивших от 2 до 4 заказов (9.900063 < 21.329939). Это может указывать на то, что более частые заказы (меньший интервал) способствуют увеличению вероятности повторного заказа. Пользователи, делающие заказы чаще, склонны совершать больше заказов в целом.
# 
# Самые сильные признаки связанные с total_orders это is_five(0.615266),first_order_date(0.429065),last_order_date(0.410308), is_two(0.309085). is_five имеет заметную корреляцию. first_order_date, last_order_date, is_two имеют умеренную корреляцию.
# 
# Рекомендации.
# 
# Стоит обратить внимание на пользователей, кто делает первый заказ через desctop, так как доля тех кто вернулся на Афишу больше чем у mobile. Имеет смысл сделать промоакции, чтобы повысить общую долю заказов через desctop. Так же нужно обратить внимание на пользователей, кто делал первый заказ с Каменевского региона и Североярской области, в них больше всего пользовател, они являются самыми крупными, но доля возвращающихся меньше. 

# ### 6. Финализация проекта и публикация в Git
# 
# Когда вы закончите анализировать данные, оформите проект, а затем опубликуйте его.
# 
# Выполните следующие действия:
# 
# 1. Создайте файл `.gitignore`. Добавьте в него все временные и чувствительные файлы, которые не должны попасть в репозиторий.
# 2. Сформируйте файл `requirements.txt`. Зафиксируйте все библиотеки, которые вы использовали в проекте.
# 3. Вынести все чувствительные данные (параметры подключения к базе) в `.env`файл.
# 4. Проверьте, что проект запускается и воспроизводим.
# 5. Загрузите проект в публичный репозиторий — например, на GitHub. Убедитесь, что все нужные файлы находятся в репозитории, исключая те, что в `.gitignore`. Ссылка на репозиторий понадобится для отправки проекта на проверку. Вставьте её в шаблон проекта в тетрадке Jupyter Notebook перед отправкой проекта на ревью.

# **Вставьте ссылку на проект в этой ячейке тетрадки перед отправкой проекта на ревью.**
